# RAG系统项目 - 基于FAISS与千问模型API

## 项目概述
本项目利用FAISS向量库和千问模型API构建了一个检索增强生成（RAG）系统，旨在为用户提供针对公司年度总结数据的智能问答服务。通过将文本数据转换为向量并建立高效索引，结合强大的语言模型生成能力，实现精准、快速的信息检索与回答生成。

## 运行步骤
1. **依赖安装**：运行[requirements.txt](requirements.txt)文件，`pip install -r requirements.txt`进行所需依赖下载。
1. **向量存储**：运行 [`RAG-faiss.py`](Codes/RAG-faiss.py) 文件，该文件负责将公司年度总结数据进行向量化处理，并使用FAISS向量库构建向量索引，将其存储于本地。这一步为后续的检索操作奠定基础。
2. **RAG对话**：执行 [`Codes/RAG.py`](Codes/RAG.py) 文件，启动RAG对话功能。在该环节，系统会根据用户输入的问题，从已建立的向量索引中检索相关信息，再结合千问模型API生成准确且有针对性的回答。

## 项目亮点
- **高效向量处理**：借助FAISS向量库，实现对大规模向量数据的高效管理与快速检索，提升系统整体性能。
- **优质模型集成**：集成千问模型API，充分利用其强大的自然语言处理能力，确保生成的回答具备高准确性和语言流畅性。

## 技术栈
- **向量处理**：FAISS（Facebook AI Similarity Search），用于向量索引构建与相似性搜索。
- **模型调用**：千问模型API，提供文本生成与理解功能。

## 注意事项
- 在运行项目前，请确保已正确配置千问模型API密钥，以及FAISS库的相关依赖。
- 若在运行过程中遇到文件大小限制问题（如向量索引文件过大），可参考相关文档使用Git LFS等工具进行处理。 


以下是我们对该RAG项目进行拆解的步骤，加深对RAG项目整体流程的理解。

## **一、知识预处理：从原始文档到可检索文本**  
**目标**：将多格式、多来源的非结构化数据转化为统一的纯文本表示，为后续向量构建奠定基础。  
- **核心组件**：  
  - **文档解析工具**：  
    - **PyMuPDF**：高效解析PDF文档，支持图文分离、表格提取，处理速度可达**100页/秒**（实测数据）；  
    - **python-docx/Markdown解析器**：解析Word、Markdown文档，保留标题层级结构（如H1/H2标签）用于语义分块；  
    - **Tika**：通用文档解析工具，支持PPT、Excel、XML等200+格式，适合企业多源数据整合场景。  
  - **文本清洗模块**：  
    - **正则表达式**：去除乱码（如`\x00`）、重复空行、HTML标签；  
    - **中文分词工具**：Jieba/THULAC，用于断句和关键词提取，提升后续Embedding语义表征精度。  
- **关键技术点**：  
  - **分块策略**：按固定长度（如500字）或语义单元（如段落、章节）分割文本，避免长文本向量语义稀释；  
  - **元数据管理**：为每个文本块附加来源（如`document_id`）、创建时间、关键词等元数据，便于回答溯源。  


## **二、向量构建：从文本到语义向量的编码**  
**目标**：将清洗后的文本转化为高维向量，通过向量索引实现快速语义检索。  
- **核心组件**：  
  - **Embedding模型**：  
    - **通义千问Embedding模型**：专为中文优化，支持长文本语义编码（最长16K tokens），余弦相似度准确率较开源模型（如BERT-base）提升15%；  
    - **开源模型**：BGE（ByteDance General Embedding）系列，如`bge-large-zh`在中文语义匹配任务中准确率达92%；  
  - **向量索引引擎**：  
    - **FAISS**：Facebook开源向量检索库，支持IVF、HNSW等高效索引结构，百万级向量检索延迟可控制在50ms以内；  
    - **Milvus**：国产分布式向量数据库，支持动态索引更新、多模态检索（文本+图像），适合高并发场景。  
- **技术优化方向**：  
  - **批量向量化**：通过`batch_size=32`等参数批量调用Embedding接口，网络延迟降低80%；  
  - **索引增量更新**：采用FAISS的`add_with_ids`接口，支持逐条/分批次更新索引，避免全量重建耗时。  


## **三、检索层：基于语义的高效知识召回**  
**目标**：根据用户问题实时检索最相关的知识片段，缩小生成层的上下文范围。  
- **核心流程**：  
  1. **问题向量化**：使用与文档相同的Embedding模型生成问题向量；  
  2. **向量检索**：通过FAISS的`search`接口执行近似最近邻搜索（ANN），返回Top-K相关文本块；  
  3. **上下文拼接**：将检索结果按相似度排序，拼接为`[问题]+[知识片段1]+[知识片段2]+...`格式的Prompt。  
- **关键参数调优**：  
  - **Top-K取值**：默认取5-10条，兼顾信息充分性与Token限制（如Qwen模型最大支持128K Token）；  
  - **索引参数**：  
    - **IVF**：`nlist=1000`平衡检索速度与精度（百万级数据场景）；  
    - **HNSW**：`M=16, ef=100`在保证速度的同时提升召回率。  


## **四、生成层：基于检索结果的智能回答**  
**目标**：结合检索到的上下文与大语言模型能力，生成准确、自然的回答。  
- **核心组件**：  
  - **通义千问对话模型（qwen-plus）**：  
    - 支持最长128K Token上下文，适合处理长篇文档分析任务；  
    - 内置`role`字段（user/assistant/system），原生支持多轮对话历史拼接。  
  - **Prompt工程**：  
    - **系统提示（System Prompt）**：引导模型行为（如*"你是一个法律助手，需根据提供的法规内容回答问题，并标注条款来源"*）；  
    - **Few-Shot示例**：在Prompt中插入1-2个历史问答对，规范回答格式（如JSON结构化输出）。  
- **输出控制**：  
  - **Token限制**：通过`max_tokens=2000`控制回答长度，避免截断关键信息；  
  - **温度参数**：`temperature=0.3`（事实性问答）或`temperature=0.8`（创意内容生成），调节回答随机性。  


## **五、工程化：从原型到生产的系统级优化**  
**目标**：提升系统性能、稳定性与可维护性，满足企业级部署要求。  
- **核心模块**：  
  1. **缓存系统（Redis）**：  
     - 缓存高频问题的Embedding向量、检索结果和生成回答，命中率可达60%-80%，降低后端压力；  
     - 支持按TTL（生存时间）自动淘汰冷数据，如设置`EXPIRE key 86400`（24小时过期）。  
  2. **异步队列（Kafka/RabbitMQ）**：  
     - 解耦请求接收与业务处理，支持削峰填谷（如突发10万QPS时缓冲处理）；  
     - 典型流程：用户请求→消息队列→异步Worker处理（Embedding生成→检索→LLM调用）→结果回调。  
  3. **监控与日志**：  
     - **Prometheus+Grafana**：监控指标包括LLM调用延迟（p99<1s）、检索QPS（目标500+）、缓存命中率；  
     - **ELK Stack**：集中管理日志，支持按`user_id`、`request_id`追踪全链路请求，故障定位时间从小时级缩短至分钟级。  
  4. **多模态扩展**：  
     - 集成图像Embedding模型（如CLIP），支持"图片+文本"混合检索，适配电商商品问答、医疗影像分析等场景。  


## **RAG 系统的企业级应用价值**  
1. **效率提升**：  
   - 客服场景：问题响应时间从人工处理的5分钟缩短至实时回复，人力成本降低70%；  
   - 文档分析：财报/合同解析效率提升10倍，关键信息提取准确率达95%以上。  
2. **成本优化**：  
   - 避免自建大模型的千万级训练成本，通过API调用按需付费；  
   - 向量索引与缓存机制使硬件资源利用率提升300%，GPU成本降低50%。  
3. **场景覆盖**：  
   - **垂直领域**：金融（合规咨询）、医疗（病历分析）、教育（智能题库）等专业场景；  
   - **通用场景**：企业内部知识库、智能客服、内容生成平台等。  


## **未来演进方向**  
1. **检索-生成联合优化**：引入`Retrieval-Augmented Transformer`架构，实现检索与生成的端到端训练，提升长上下文相关性；  
2. **可信AI能力**：集成`Chain of Thought（CoT）`推理链，实现回答可解释性；通过数字水印技术防止生成内容滥用；  
3. **边缘计算部署**：基于`ONNX Runtime`轻量化模型，支持本地化部署，满足离线环境或数据隐私要求高的场景（如政务、军工）。  


## **总结**  
RAG 系统通过 **“数据预处理的标准化、向量检索的高效化、大模型生成的智能化、工程架构的弹性化”** 四层技术突破，构建了兼具准确性与可扩展性的智能问答解决方案。企业可根据自身数据规模（如百万级文档选择IVF索引，亿级数据采用分布式Milvus）、业务场景（如实时交互选择异步队列）和成本预算（如本地化部署Embedding模型），灵活裁剪技术栈组件，快速落地符合需求的智能知识服务系统。
